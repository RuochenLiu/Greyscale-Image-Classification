knitr::opts_chunk$set(echo = TRUE)
img_dir <- "../data/raw_images/"
dir_names <- list.files(img_dir)
library("EBImage")
library("OpenImageR")
install.packages("OpenInamgeR")
library("EBImage")
library("OpenImageR")
install.packages("OpenImageR")
library("EBImage")
library("OpenImageR")
n_files <- length(list.files(img_dir))
### determine img dimensions
img0 <- readImage(paste0(img_dir,  dir_names[1]))
display(img0)
img1 <- readImage(paste0(img_dir,  dir_names[2]))
img2 <-  readImage(paste0(img_dir,  dir_names[3]))
h1 <- HOG(img2)
### store vectorized pixel values of images
dat <- matrix(NA, n_files,54)
for(i in 1:n_files){
img <- readImage(paste0(img_dir, dir_names[i]))
h <- HOG(img)
dat[i,] <- h
}
### output constructed features
if(export){
save(dat, file=paste0("../output/HOG.RData"))
}
library("caret")
library("gbm")
library("randomForest")
library("plyr")
library("xgboost")
library("fastAdaboost")
library("e1071")
setwd("~/GitHub/spr2017-proj3-group7")
sift_f <- read.csv("./data/sift_features.csv")
X <- t(sift_f)
labels <- read.csv("./data/labels.csv")
labels <- labels[,1]
dat_train<-X
label_train <- labels
predict_results = vector()
GBMTrain <- function(dat_train, label_train){
n <- nrow(dat_train)
r <- sample(1:n, 0.1*n)
dat_train <- dat_train[r,]
label_train <- label_train[r]
data.all <- as.data.frame(cbind(dat_train, label_train))
colnames(data.all)[ncol(data.all)] <- "Label"
data.all$Label <- as.factor(data.all$Label)
control <- trainControl(method = 'cv', number = 5)  #use 5-fold cross validation
inTrain <- createDataPartition(y = data.all$Label, p=0.75, list=FALSE)
training <- data.all[inTrain, ]
testing <- data.all[-inTrain, ]
# GBM
gbmGrid <- expand.grid(interaction.depth = (1:5) * 2,n.trees = (1:10)*25,shrinkage = .1,
n.minobsinnode = 10)
gbmfit <- train(Label~., data = training,
method = "gbm", trControl = control, verbose = FALSE,
bag.fraction = 0.5, tuneGrid = gbmGrid
) #parameter tuning
err.gbm <- sum(predict(gbmfit, testing) != testing$Label)/nrow(testing)
print(paste("gbm error is",err.gbm))
predict_results[1]=err.gbm
}
t1<-Sys.time()
GBMTrain(cbind(dat,X),labels)
t2<-Sys.time()
print(paste("GBM time is",difftime(t2,t1)))
t1<-Sys.time()
GBMTrain(dat,labels)
t2<-Sys.time()
print(paste("GBM time is",difftime(t2,t1)))
t1<-Sys.time()
GBMTrain(dat,labels)
t2<-Sys.time()
print(paste("GBM time is",difftime(t2,t1)))
img_dir <- "../data/raw_images/"
dir_names <- list.files(img_dir)
n_files <- length(list.files(img_dir))
img0 <- readImage(paste0(img_dir,  dir_names[1]))
h1 <- HOG(img0)
library("EBImage")
library("OpenImageR")
img_dir <- "../data/raw_images/"
dir_names <- list.files(img_dir)
n_files <- length(list.files(img_dir))
img0 <- readImage(paste0(img_dir,  dir_names[1]))
h1 <- HOG(img0)
H <- matrix(NA, n_files,54)
for(i in 1:n_files){
img <- readImage(paste0(img_dir, dir_names[i]))
h <- HOG(img)
dat[i,] <- h
}
for(i in 1:n_files){
img <- readImage(paste0(img_dir, dir_names[i]))
h <- HOG(img)
H[i,] <- h
}
if(export){
save(H, file=paste0("../output/HOG.RData"))
}
return(H)
export <- TRUE
if(export){
save(H, file=paste0("../output/HOG.RData"))
}
return(H)
dim(H)
l <- List_2_Array(img0)
l <- ZCAwhiten(img0)
l <- invariant_hash(img0)
X <- read.csv("../data/sift_features.csv")
X <- t(X)
load("../output/HOG.RData")
labels <- read.csv("../data/labels.csv")
labels <- labels[,1]
dat_train<-H
label_train <- labels
library("caret")
library("gbm")
library("randomForest")
library("plyr")
library("xgboost")
library("fastAdaboost")
library("e1071")
library("deepboost")
library("kernlab")
n <- nrow(dat_train)
r <- sample(1:n, n)
dat_train <- dat_train[r,]
label_train <- label_train[r]
data.all <- as.data.frame(cbind(dat_train, label_train))
colnames(data.all)[ncol(data.all)] <- "Label"
data.all$Label <- as.factor(data.all$Label)
control <- trainControl(method = 'cv', number = 5)  #use 5-fold cross validation
inTrain <- createDataPartition(y = data.all$Label, p=0.75, list=FALSE)
training <- data.all[inTrain, ]
testing <- data.all[-inTrain, ]
gbmGrid <- expand.grid(interaction.depth = (1:5) * 2,n.trees = (1:10)*25,shrinkage = .1,
n.minobsinnode = 10)
gbmfit <- train(Label~., data = training,
method = "gbm", trControl = control, verbose = FALSE,
bag.fraction = 0.5, tuneGrid = gbmGrid) #parameter tuning
Train <- function(dat_train, label_train){
n <- nrow(dat_train)
r <- sample(1:n, n)
dat_train <- dat_train[r,]
label_train <- label_train[r]
data.all <- as.data.frame(cbind(dat_train, label_train))
colnames(data.all)[ncol(data.all)] <- "Label"
data.all$Label <- as.factor(data.all$Label)
control <- trainControl(method = 'cv', number = 5)  #use 5-fold cross validation
inTrain <- createDataPartition(y = data.all$Label, p=0.75, list=FALSE)
training <- data.all[inTrain, ]
testing <- data.all[-inTrain, ]
##### GBM ######
gbmGrid <- expand.grid(interaction.depth = (1:5) * 2,n.trees = (1:10)*25,shrinkage = .1,
n.minobsinnode = 10)
gbmfit <- train(Label~., data = training,
method = "gbm", trControl = control, verbose = FALSE,
bag.fraction = 0.5, tuneGrid = gbmGrid) #parameter tuning
train.gbm <- sum(predict(gbmfit, training) != training$Label)/nrow(training)
test.gbm <- sum(predict(gbmfit, testing) != testing$Label)/nrow(testing)
###### Random Forest ######
rfGrid <- expand.grid(mtry = 2^(1:5) )
rffit <- train(Label~., data = training,
method = "rf", trControl = control, tuneGrid = rfGrid) #parameter tuning
train.rf <- sum(predict(rffit, training) != training$Label)/nrow(training)
test.rf <- sum(predict(rffit, testing) != testing$Label)/nrow(testing)
###### SVM with Linear Kernel ######
svmGrid.linear <- expand.grid(C= 2^c(0:5))
svm.linear <- train(Label~., data = training,
method = "svmLinear", trControl = control, tuneGrid = svmGrid.linear, preProc = c("center","scale")
)
train.svmlinear <- sum(predict(svm.linear, training) != training$Label)/nrow(training)
test.svmlinear <- sum(predict(svm.linear, testing) != testing$Label)/nrow(testing)
###### SVM with RBF Kernel ######
svmGrid <- expand.grid(sigma= 2^c(-25, -20, -15,-10, -5, 0), C= 2^c(0:5))
svmfit <- train(Label~., data = training,
method = "svmRadial", trControl = control, tuneGrid = svmGrid, preProc = c("center","scale")
)
train.svm <- sum(predict(svmfit, training) != training$Label)/nrow(training)
test.svm <- sum(predict(svmfit, testing) != testing$Label)/nrow(testing)
###### xgBoost ######
xgbfit <- train(Label~., data = training,
method = "xgbLinear", trControl = control)
train.xgb <- sum(predict(xgbfit, training) != training$Label)/nrow(training)
test.xgb <- sum(predict(xgbfit, testing) != testing$Label)/nrow(testing)
###### DeepBoost ######
#dbfit <- train(Label~., data = training,
#               method = "deepboost", trControl = control, verbose = FALSE)
#
#train.db <- sum(predict(dbfit, training) != training$Label)/nrow(training)
#
#test.db <- sum(predict(dbfit, testing) != testing$Label)/nrow(testing)
#Summarize training and test errors for all models
#training.err.plot<-plot(unlist(training.error))
#test.err.plot<-plot(unlist(test.error))
models <- list(GBM = gbmfit, RF = rffit, SVML = svm.linear, SVM = svmfit, XGB = xgbfit)
train.error <- c(train.gbm, train.rf, train.svmlinear, train.svm, train.xgb)
test.error <- c(test.gbm, test.rf, test.svmlinear, test.svm, test.xgb)
errors <- data.frame(Train = train.error,Test = test.error)
rownames(errors) <- c("GBM","Random Forest","Linear SVM", "SVM with RBF","xgBoost")
return(list(models = models, errors = errors))
}
n <- nrow(dat_train)
train.gbm <- sum(predict(gbmfit, training) != training$Label)/nrow(training)
test.gbm <- sum(predict(gbmfit, testing) != testing$Label)/nrow(testing)
rfGrid <- expand.grid(mtry = 2^(1:5) )
rffit <- train(Label~., data = training,
method = "rf", trControl = control, tuneGrid = rfGrid) #parameter tuning
train.rf <- sum(predict(rffit, training) != training$Label)/nrow(training)
test.rf <- sum(predict(rffit, testing) != testing$Label)/nrow(testing)
svmGrid.linear <- expand.grid(C= 2^c(0:5))
svm.linear <- train(Label~., data = training,
method = "svmLinear", trControl = control, tuneGrid = svmGrid.linear, preProc = c("center","scale")
)
train.svmlinear <- sum(predict(svm.linear, training) != training$Label)/nrow(training)
test.svmlinear <- sum(predict(svm.linear, testing) != testing$Label)/nrow(testing)
svmGrid <- expand.grid(sigma= 2^c(-25, -20, -15,-10, -5, 0), C= 2^c(0:5))
svmfit <- train(Label~., data = training,
method = "svmRadial", trControl = control, tuneGrid = svmGrid, preProc = c("center","scale")
)
train.svm <- sum(predict(svmfit, training) != training$Label)/nrow(training)
test.svm <- sum(predict(svmfit, testing) != testing$Label)/nrow(testing)
xgbfit <- train(Label~., data = training,
method = "xgbLinear", trControl = control)
train.xgb <- sum(predict(xgbfit, training) != training$Label)/nrow(training)
test.xgb <- sum(predict(xgbfit, testing) != testing$Label)/nrow(testing)
models <- list(GBM = gbmfit, RF = rffit, SVML = svm.linear, SVM = svmfit, XGB = xgbfit)
train.error <- c(train.gbm, train.rf, train.svmlinear, train.svm, train.xgb)
test.error <- c(test.gbm, test.rf, test.svmlinear, test.svm, test.xgb)
errors <- data.frame(Train = train.error,Test = test.error)
rownames(errors) <- c("GBM","Random Forest","Linear SVM", "SVM with RBF","xgBoost")
View(errors)
gbmGrid <- expand.grid(interaction.depth = (1:5),n.trees = (1:10),shrinkage = .1,
n.minobsinnode = 10)
gbmfit <- train(Label~., data = training,
method = "gbm", trControl = control, verbose = FALSE,
bag.fraction = 0.5, tuneGrid = gbmGrid) #parameter tuning
train.gbm <- sum(predict(gbmfit, training) != training$Label)/nrow(training)
test.gbm <- sum(predict(gbmfit, testing) != testing$Label)/nrow(testing)
rfGrid <- expand.grid(mtry = 5:10)
rffit <- train(Label~., data = training,
method = "rf", trControl = control, tuneGrid = rfGrid) #parameter tuning
train.rf <- sum(predict(rffit, training) != training$Label)/nrow(training)
test.rf <- sum(predict(rffit, testing) != testing$Label)/nrow(testing)
rfGrid <- expand.grid(mtry = 1:5)
rffit <- train(Label~., data = training,
method = "rf", trControl = control, tuneGrid = rfGrid) #parameter tuning
train.rf <- sum(predict(rffit, training) != training$Label)/nrow(training)
test.rf <- sum(predict(rffit, testing) != testing$Label)/nrow(testing)
svmGrid <- expand.grid(sigma= 2^c(-4,-3,-2,-1,0), C= (1:5))
svmfit <- train(Label~., data = training,
method = "svmRadial", trControl = control, tuneGrid = svmGrid, preProc = c("center","scale")
)
train.svm <- sum(predict(svmfit, training) != training$Label)/nrow(training)
test.svm <- sum(predict(svmfit, testing) != testing$Label)/nrow(testing)
svmGrid.linear <- expand.grid(C= 2^c(0,1,2,-1,-2))
svm.linear <- train(Label~., data = training,
method = "svmLinear", trControl = control, tuneGrid = svmGrid.linear, preProc = c("center","scale")
)
train.svmlinear <- sum(predict(svm.linear, training) != training$Label)/nrow(training)
test.svmlinear <- sum(predict(svm.linear, testing) != testing$Label)/nrow(testing)
svmGrid <- expand.grid(sigma= 2^c(-4,-3,-2,-1,0), C= (1:5))
svmfit <- train(Label~., data = training,
method = "svmRadial", trControl = control, tuneGrid = svmGrid, preProc = c("center","scale")
)
train.svm <- sum(predict(svmfit, training) != training$Label)/nrow(training)
test.svm <- sum(predict(svmfit, testing) != testing$Label)/nrow(testing)
?randomForest
gbmGrid <- expand.grid(interaction.depth = (1:5),n.trees = (1:15),shrinkage = .1,
n.minobsinnode = 10)
gbmfit <- train(Label~., data = training,
method = "gbm", trControl = control, verbose = FALSE,
bag.fraction = 0.5, tuneGrid = gbmGrid) #parameter tuning
train.gbm <- sum(predict(gbmfit, training) != training$Label)/nrow(training)
test.gbm <- sum(predict(gbmfit, testing) != testing$Label)/nrow(testing)
gbmGrid <- expand.grid(interaction.depth = (1:5),n.trees = (1:20),shrinkage = .1,
n.minobsinnode = 10)
gbmfit <- train(Label~., data = training,
method = "gbm", trControl = control, verbose = FALSE,
bag.fraction = 0.5, tuneGrid = gbmGrid) #parameter tuning
train.gbm <- sum(predict(gbmfit, training) != training$Label)/nrow(training)
test.gbm <- sum(predict(gbmfit, testing) != testing$Label)/nrow(testing)
gbmGrid <- expand.grid(interaction.depth = (1:8),n.trees = (1:10),shrinkage = .1,
n.minobsinnode = 10)
gbmfit <- train(Label~., data = training,
method = "gbm", trControl = control, verbose = FALSE,
bag.fraction = 0.5, tuneGrid = gbmGrid) #parameter tuning
train.gbm <- sum(predict(gbmfit, training) != training$Label)/nrow(training)
test.gbm <- sum(predict(gbmfit, testing) != testing$Label)/nrow(testing)
svmGrid.linear <- expand.grid(C= 2^c(0,1,2,-1,-2))
svm.linear <- train(Label~., data = training,
method = "svmLinear", trControl = control, tuneGrid = svmGrid.linear, preProc = c("center","scale")
)
train.svmlinear <- sum(predict(svm.linear, training) != training$Label)/nrow(training)
test.svmlinear <- sum(predict(svm.linear, testing) != testing$Label)/nrow(testing)
gbmGrid <- expand.grid(interaction.depth = (1:10),n.trees = (1:10),shrinkage = .1,
n.minobsinnode = 10)
gbmfit <- train(Label~., data = training,
method = "gbm", trControl = control, verbose = FALSE,
bag.fraction = 0.5, tuneGrid = gbmGrid) #parameter tuning
train.gbm <- sum(predict(gbmfit, training) != training$Label)/nrow(training)
test.gbm <- sum(predict(gbmfit, testing) != testing$Label)/nrow(testing)
svmGrid.linear <- expand.grid(C= 2^c(0,1,2,-1,-2))
svm.linear <- train(Label~., data = training,
method = "svmLinear", trControl = control, tuneGrid = svmGrid.linear, preProc = c("center","scale")
)
train.svmlinear <- sum(predict(svm.linear, training) != training$Label)/nrow(training)
test.svmlinear <- sum(predict(svm.linear, testing) != testing$Label)/nrow(testing)
svmGrid <- expand.grid(sigma= 2^c(-4,-3,-2,-1,0), C= 2^c(0,1,2,-1,-2))
svmfit <- train(Label~., data = training,
method = "svmRadial", trControl = control, tuneGrid = svmGrid, preProc = c("center","scale")
)
train.svm <- sum(predict(svmfit, training) != training$Label)/nrow(training)
test.svm <- sum(predict(svmfit, testing) != testing$Label)/nrow(testing)
svmGrid <- expand.grid(sigma= 2^c(1,2,-2,-1,0), C= 2^c(0,1,2,-1,-2))
svmfit <- train(Label~., data = training,
method = "svmRadial", trControl = control, tuneGrid = svmGrid, preProc = c("center","scale")
)
train.svm <- sum(predict(svmfit, training) != training$Label)/nrow(training)
test.svm <- sum(predict(svmfit, testing) != testing$Label)/nrow(testing)
svmGrid <- expand.grid(sigma= 2^c(-25, -20, -15,-10, -5, 0), C= 2^c(0:5))
svmfit <- train(Label~., data = training,
method = "svmRadial", trControl = control, tuneGrid = svmGrid, preProc = c("center","scale")
)
train.rf <- sum(predict(rffit, training) != training$Label)/nrow(training)
test.rf <- sum(predict(rffit, testing) != testing$Label)/nrow(testing)
svmGrid <- expand.grid(sigma= 2^c(-25, -20, -15,-10, -5, 0), C= 2^c(0:5))
svmfit <- train(Label~., data = training,
method = "svmRadial", trControl = control, tuneGrid = svmGrid, preProc = c("center","scale")
)
train.svm <- sum(predict(svmfit, training) != training$Label)/nrow(training)
test.svm <- sum(predict(svmfit, testing) != testing$Label)/nrow(testing)
svmGrid.linear <- expand.grid(C= 2^c(0,1,2,-1,-2))
svm.linear <- train(Label~., data = training,
method = "svmLinear", trControl = control, tuneGrid = svmGrid.linear, preProc = c("center","scale")
)
train.svmlinear <- sum(predict(svm.linear, training) != training$Label)/nrow(training)
test.svmlinear <- sum(predict(svm.linear, testing) != testing$Label)/nrow(testing)
Train <- function(dat_train, label_train){
n <- nrow(dat_train)
r <- sample(1:n, n)
dat_train <- dat_train[r,]
label_train <- label_train[r]
data.all <- as.data.frame(cbind(dat_train, label_train))
colnames(data.all)[ncol(data.all)] <- "Label"
data.all$Label <- as.factor(data.all$Label)
control <- trainControl(method = 'cv', number = 10)  #use 5-fold cross validation
inTrain <- createDataPartition(y = data.all$Label, p=0.75, list=FALSE)
training <- data.all[inTrain, ]
testing <- data.all[-inTrain, ]
##### GBM ######
gbmGrid <- expand.grid(interaction.depth = (1:10),n.trees = (1:10),shrinkage = .1,
n.minobsinnode = 10)
gbmfit <- train(Label~., data = training,
method = "gbm", trControl = control, verbose = FALSE,
bag.fraction = 0.5, tuneGrid = gbmGrid) #parameter tuning
train.gbm <- sum(predict(gbmfit, training) != training$Label)/nrow(training)
test.gbm <- sum(predict(gbmfit, testing) != testing$Label)/nrow(testing)
###### Random Forest ######
rfGrid <- expand.grid(mtry = 1:5)
rffit <- train(Label~., data = training,
method = "rf", trControl = control, tuneGrid = rfGrid) #parameter tuning
train.rf <- sum(predict(rffit, training) != training$Label)/nrow(training)
test.rf <- sum(predict(rffit, testing) != testing$Label)/nrow(testing)
###### SVM with Linear Kernel ######
svmGrid.linear <- expand.grid(C= 2^c(0,1,2,-1,-2))
svm.linear <- train(Label~., data = training,
method = "svmLinear", trControl = control, tuneGrid = svmGrid.linear, preProc = c("center","scale")
)
train.svmlinear <- sum(predict(svm.linear, training) != training$Label)/nrow(training)
test.svmlinear <- sum(predict(svm.linear, testing) != testing$Label)/nrow(testing)
###### SVM with RBF Kernel ######
svmGrid <- expand.grid(sigma= 2^c(-25, -20, -15,-10, -5, 0), C= 2^c(0:5))
svmfit <- train(Label~., data = training,
method = "svmRadial", trControl = control, tuneGrid = svmGrid, preProc = c("center","scale")
)
train.svm <- sum(predict(svmfit, training) != training$Label)/nrow(training)
test.svm <- sum(predict(svmfit, testing) != testing$Label)/nrow(testing)
###### xgBoost ######
xgbfit <- train(Label~., data = training,
method = "xgbLinear", trControl = control)
train.xgb <- sum(predict(xgbfit, training) != training$Label)/nrow(training)
test.xgb <- sum(predict(xgbfit, testing) != testing$Label)/nrow(testing)
###### DeepBoost ######
#dbfit <- train(Label~., data = training,
#               method = "deepboost", trControl = control, verbose = FALSE)
#
#train.db <- sum(predict(dbfit, training) != training$Label)/nrow(training)
#
#test.db <- sum(predict(dbfit, testing) != testing$Label)/nrow(testing)
#Summarize training and test errors for all models
#training.err.plot<-plot(unlist(training.error))
#test.err.plot<-plot(unlist(test.error))
models <- list(GBM = gbmfit, RF = rffit, SVML = svm.linear, SVM = svmfit, XGB = xgbfit)
train.error <- c(train.gbm, train.rf, train.svmlinear, train.svm, train.xgb)
test.error <- c(test.gbm, test.rf, test.svmlinear, test.svm, test.xgb)
errors <- data.frame(Train = train.error,Test = test.error)
rownames(errors) <- c("GBM","Random Forest","Linear SVM", "SVM with RBF","xgBoost")
return(list(models = models, errors = errors))
}
